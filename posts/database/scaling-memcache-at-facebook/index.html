<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Scaling Memcache At Facebook - Changhoi Kim</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="white"><meta name="application-name" content="Changhoi Kim"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="msapplication-TileColor" content="white"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Changhoi Kim"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="이 논문은 Planet Scale 서비스 중 하나인 Facebook(이하 Meta, 메타, 페이스북)이 어떻게 Memcache를 사용했는지에 대한 논문인데, 이 글은 이 논문 내용 중 “확장되는 스케일에서 어떻게 Data Consistency를 유지 했는가?”에 집중해 정리했다.  논문에서는 Memcache와 Memcached 용어를 철저히 분리한다. 전자"><meta property="og:type" content="blog"><meta property="og:title" content="Scaling Memcache At Facebook"><meta property="og:url" content="https://changhoi.kim/posts/database/scaling-memcache-at-facebook/"><meta property="og:site_name" content="Changhoi Kim"><meta property="og:description" content="이 논문은 Planet Scale 서비스 중 하나인 Facebook(이하 Meta, 메타, 페이스북)이 어떻게 Memcache를 사용했는지에 대한 논문인데, 이 글은 이 논문 내용 중 “확장되는 스케일에서 어떻게 Data Consistency를 유지 했는가?”에 집중해 정리했다.  논문에서는 Memcache와 Memcached 용어를 철저히 분리한다. 전자"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://changhoi.kim/images/2023-06-23-scaling-memcache-at-facebook/thumbnail.png?style=centerme"><meta property="article:published_time" content="2023-06-22T15:00:00.000Z"><meta property="article:modified_time" content="2023-06-22T15:00:00.000Z"><meta property="article:author" content="changhoi"><meta property="article:tag" content="system_design"><meta property="article:tag" content="memcache"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://changhoi.kim/images/2023-06-23-scaling-memcache-at-facebook/thumbnail.png?style=centerme"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://changhoi.kim/posts/database/scaling-memcache-at-facebook/"},"headline":"Scaling Memcache At Facebook","image":[],"datePublished":"2023-06-22T15:00:00.000Z","dateModified":"2023-06-22T15:00:00.000Z","author":{"@type":"Person","name":"changhoi"},"publisher":{"@type":"Organization","name":"Changhoi Kim","logo":{"@type":"ImageObject","url":"https://changhoi.kim/img/logo.jpg"}},"description":"이 논문은 Planet Scale 서비스 중 하나인 Facebook(이하 Meta, 메타, 페이스북)이 어떻게 Memcache를 사용했는지에 대한 논문인데, 이 글은 이 논문 내용 중 “확장되는 스케일에서 어떻게 Data Consistency를 유지 했는가?”에 집중해 정리했다.  논문에서는 Memcache와 Memcached 용어를 철저히 분리한다. 전자"}</script><link rel="canonical" href="https://changhoi.kim/posts/database/scaling-memcache-at-facebook/"><link rel="alternate" href="/rss2.xml" title="Changhoi Kim" type="application/atom+xml"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-QBSLW6WTZW" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-QBSLW6WTZW');</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.jpg" alt="Changhoi Kim" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/images/2023-06-23-scaling-memcache-at-facebook/thumbnail.png?style=centerme" alt="Scaling Memcache At Facebook"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2023-06-22T15:00:00.000Z" title="2023. 6. 22. 오전 8:00:00">2023-06-23</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2023-06-22T15:00:00.000Z" title="2023. 6. 22. 오전 8:00:00">2023-06-23</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">38분안에 읽기 (약 5685 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">Scaling Memcache At Facebook</h1><div class="content"><p>이 논문은 Planet Scale 서비스 중 하나인 Facebook(이하 Meta, 메타, 페이스북)이 어떻게 Memcache를 사용했는지에 대한 논문인데, 이 글은 이 논문 내용 중 “확장되는 스케일에서 어떻게 Data Consistency를 유지 했는가?”에 집중해 정리했다.</p>
<blockquote>
<p>논문에서는 <code>Memcache</code>와 <code>Memcached</code> 용어를 철저히 분리한다. 전자는 분산 시스템을 구성하는 시스템 자체를 의미하고 후자는 실행되는 서버, 바이너리 자체를 의미한다.</p>
</blockquote>
<span id="more"></span>

<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>메타에서는 캐시를 정말 대규모로 사용한다. 생각해 보면 페이스북은 캐시를 쓰기에 가장 적합한 유즈 케이스를 가지고 있다. 일단 논문에서 말하는 용례는 다음과 같다.</p>
<ul>
<li><strong>Query Cache</strong>: 데이터베이스 읽기 부하를 줄이기 위해 사용한다. 특히 <code>demand-filled look-aside</code> 캐시로 사용한다. </li>
<li><strong>Generic Cache</strong>: 굉장히 일반적인 용례를 의미한다. 거의 나머지라고 보면 될 수준. 연산이 오래 걸리는 결과(ex. 머신 러닝 결과 등)라든지 어찌 됐든 무거운 무언가를 해야 하는 걸 담아두고 여러 서비스에서 꺼내서 쓰는 용도이다.</li>
</ul>
<blockquote>
<p><code>demand-filled look-aside</code> 캐시는 흔히 우리가 알고 있는 캐싱 방법이다. 읽어올 때 캐시를 확인 먼저하고 없으면 Origin 데이터 소스로부터 값을 가져오는 방식을 의미한다. 논문과 영상에서 짧게 나오는 내용 중에 <code>look-aside</code> 캐시를 만들기 위해서 Origin과 동기화를 위해 데이터 소스의 변경이 발생하면 캐시의 데이터를 수정하지 않고 삭제하는 방법을 선택했다고 한다. 보통 Cache Invalidation이 이렇게 동작하기 때문에 일반적인 것 같지만, 아무튼 삭제를 선택한 이유는 수정보다 멱등적이기 때문이라고 설명한다.</p>
</blockquote>
<p>이 논문에서는 위 용례에 대해 (보통 Query Cache에 대한 내용인 듯) 배포 스케일에 따라 마주한 공학적 어려움을 설명해주고 있다. 이를 크게 세 단계로 나눠서 설명한다.</p>
<ol>
<li>단일한 클러스터</li>
<li>여러 개의 Front-End(이하 FE, 프론트엔드) 클러스터 </li>
<li>세계 단위로 여러 클러스터를 두는 상황</li>
</ol>
<h1 id="Single-Cluster"><a href="#Single-Cluster" class="headerlink" title="Single Cluster"></a>Single Cluster</h1><p><img src="/images/2023-06-23-scaling-memcache-at-facebook/single-cluster.png?style=centerme" alt="단일 클러스터"><br>이 수준에서는 지연을 줄이거나 Cache Miss로 인해 발생하는 부하를 줄이기 위해 노력하는 스케일이다. 이 장에서는 지연을 줄이기 위한 방법, 부하를 줄이기 위한 방법, 실패 처리에 대해 자세히 설명한다.</p>
<blockquote>
<p>이 글에서 일관성 얘기를 위해 적합한 스테이지가 아니다. 만약 일관성 문제만 궁금하다면 멀티 클러스터 단위로 넘어가도 좋다.</p>
</blockquote>
<h2 id="지연-줄이기"><a href="#지연-줄이기" class="headerlink" title="지연 줄이기"></a>지연 줄이기</h2><p>우선 클러스터로 운영하고 있는 <code>Memcache</code>의 상황을 설명하자면, 수 백대의 <code>Memcached</code> 서버에 데이터가 Consistent Hashing으로 분산되어 있다. 또한 웹서버가 하나의 페이지를 만들기 위해 수많은 <code>Memcached</code>로부터 동시에 값을 읽어오게 된다. 예를 들어 인기 있는 페이지의 결과를 위해 평균적으로 521개의 독립적인 아이템을 <code>Memcached</code>에서 가져온다고 한다. 이러한 이유로 클라이언트는 짧은 시간에 엄청난 양의 데이터 응답을 받을 수 있는 상황이다. 이렇게 되면 다음과 같은 문제가 발생할 수 있다.</p>
<ul>
<li>하나의 <code>Memcached</code> 병목이 웹서버의 병목 지점이 될 수 있다.</li>
<li>웹서버에 <code>Incast Congestion</code>이 발생할 수 있다.</li>
<li>데이터 복제를 통해 단일 서버 병목을 완화할 수 있지만 데이터 비효율을 감수해야 한다.</li>
</ul>
<blockquote>
<p><code>Incast Congestion</code>은 TCP 응답이 과도하게 몰려 TCP 윈도우를 압도하는 상황을 의미한다. 이 상황이 되면 패킷이 드랍되는 등 느려지는 원인이 될 수 있다.</p>
</blockquote>
<p>이러한 문제를 메타에서는 <code>Memcache</code> 클라이언트를 개선해 해결했다. </p>
<h3 id="Parallel-Requests-And-Batching"><a href="#Parallel-Requests-And-Batching" class="headerlink" title="Parallel Requests And Batching"></a>Parallel Requests And Batching</h3><p>Directed Acyclic Graph(DAG)를 그려 데이터 사이의 의존성을 확인한 후 웹서버가 동시에 Fetching할 수 있는 데이터를 최대로 뽑아낼 수 있게 만들었다. 이 구조로 동작하는 배치는 요청당 평균 24개의 키를 동시에 쿼리하게 되었다. 결과적으로 웹서버의 라운드 트립을 최소화하게 되었다.</p>
<h3 id="Client-Server-Communication"><a href="#Client-Server-Communication" class="headerlink" title="Client-Server Communication"></a>Client-Server Communication</h3><p><code>Memcached</code> 서버는 각자와 커뮤니케이 하지 않는데, 시스템의 복잡성을 클라이언트에 주입했다. 이로써 <code>Memcached</code> 서버는 굉장히 단순하게 유지된다.</p>
<p>클라이언트는 <code>GET</code> 요청을 보낼 때 지연과 오버헤드를 줄이기 위해 UDP를 사용한다. 클라이언트는 시퀀스 넘버를 통해 UDP 요청에 문제가 있는지 확인할 수는 있지만 Recover 하지는 않는다. 이러한 경우는 그냥 Cache Miss와 동일하게 처리된다. 이러한 방법은 경험적으로는 굉장히 실용적이었다고 한다.</p>
<blockquote>
<p>UDP 요청은 실제로 20%의 지연을 줄여주었다고 한다.</p>
</blockquote>
<p>신뢰성을 위해 <code>PUT</code> &amp; <code>DELETE</code> 요청은 여전히 TCP를 사용한다. 이를 처리하는 컴포넌트로 <code>mcrouter</code>가 있는데, 이는 Proxy로 동작하거나 라이브러리로 클라이언트에 삽입되기도 한다. 이 프록시가 클라이언트와 <code>Memcached</code>의 TCP 컨넥션을 줄여주는 역할을 함으로써 CPU, Memory, Network를 아꼈다.</p>
<h3 id="Incast-Congestion"><a href="#Incast-Congestion" class="headerlink" title="Incast Congestion"></a>Incast Congestion</h3><p>클라이언트는 TCP Incast Congestion 문제를 보완하기 위해 자체적인 혼잡 제어 메커니즘을 가지고 있었다. 클라이언트는 Sliding Window(슬라이딩 윈도우) 방법을 사용해 요청 숫자를 제어했다. 슬라이딩 윈도우 방법은 말 그대로 TCP 혼잡 제어 방법처럼 천천히 증가하다가 문제가 생기면 줄어드는 구조이다. 윈도우 사이즈를 최적화하는 것도 중요한 문제였는데, 윈도우가 커지면 Incast Congestion을 막을 수 없어 성능 저하가 발생하고, 윈도우가 작아지면 요청들의 대기 시간이 길어졌다. 이것 역시 메타에서는 경험으로 적절한 수치를 찾은 것으로 보인다.</p>
<h2 id="부하-줄이기"><a href="#부하-줄이기" class="headerlink" title="부하 줄이기"></a>부하 줄이기</h2><p>부하를 줄이기 위한 노력으로 세 가지를 소개하고 있다.</p>
<ul>
<li><strong>Lease</strong></li>
<li><strong>Memcache Pools</strong></li>
<li><strong>Replication Within Pools</strong></li>
</ul>
<h3 id="Lease"><a href="#Lease" class="headerlink" title="Lease"></a>Lease</h3><p>캐시를 사용하면서 생길 수 있는 문제로 오래된 데이터를 캐시에서 보관하는 Stale Set 문제와 특정 키가 굉장히 활발히 수정되고 읽히는 Thundering Herds 문제가 있는데, 메타는 위 문제들을 해결하기 위해 Lease 기술을 사용했다. <code>Memcached</code>는 클라이언트가 Cache Missing을 경험했을 때 데이터를 다시 채우기 위해 Lease 토큰을 발급해준다. 이 토큰은 64비트의 키 마다 유일한 값이다. 클라이언트는 캐시에 값을 저장할 때 Lease 토큰을 제공해야 한다. <code>Memcached</code>는 이를 확인하고 데이터가 저장되어야 하는지를 결정한다. 이때 “확인”(Verification) 과정은 말 그대로 “이 토큰이 특정 키에 대해 유효한가”를 보는 것이다. 예를 들어 <code>Memcached</code>가 요청을 받기 전에 해당 아이템을 삭제하라는 요청을 처리한 경우 토큰이 유효하지 않게 된 것이다.</p>
<p>이 동작 방식이 <strong>Load-Link&#x2F;Store-Conditional</strong>이라고 하는 방식과 유사하게 동작하여 동시 쓰기로 인한 과거 데이터가 쓰이는 것을 막아준다. Load-Link&#x2F;Store-Conditional 방법은 쉽게 말해 특정 값에 대한 쓰기 A가 수행되기 전에 다른 요청 B에 의해 처리되어버리면 A가 실패하도록 하는 로직이다.</p>
<p>Lease를 통해 Thundering Herds를 완화할 수도 있다. 각 <code>Memcached</code> 서버는 토큰 반환 속도를 조절할 수 있다. 기본으로 페이스북은 <code>Memcached</code>가 토큰을 10초마다 한 번 반환하도록 조정했다. 토큰 발급 후 10초 이내 값을 요구하는 경우 클라이언트에게 잠시 기다리라는 알람을 보낸다. 일반적으로 쓰기는 수 미리 초 안에 수행되기 때문에 10초 뒤의 클라이언트 요청은 데이터가 캐시에 존재하는 상황일 가능성이 높다. 하지만 이 동작은 선택적이고 만약 오래된 데이터를 어느 정도 감안하는 서비스라면 오래된 데이터일 수도 있지만 값을 리턴해준다.</p>
<h3 id="Memcache-Pools"><a href="#Memcache-Pools" class="headerlink" title="Memcache Pools"></a>Memcache Pools</h3><p>위에서 언급한 Generic Cache로 사용할 때 여러 애플리케이션에 의해 사용되면 각 서비스가 다른 목적으로 접근하고 각 서비스에서 원하는 퀄리티 수준도 모두 다르다. 이는 사용 방법에서도 차이가 크게 생기기 때문에 Cache Hit을 줄이는 결과로 이어질 수가 있다. 이 차이를 해결하기 위해 <code>Memcached</code> 서버들을 다른 성격의 풀로 나눴다. 기본적으로 디폴트에 해당하는 풀이 있는데 이 풀을 <strong>WildCard</strong>라고 한다. 그리고 이 기본 풀에 있을 때 문제가 되는 키를 복수의 다른 풀에 분배하는 구조이다.</p>
<p>예를 들어서 자주 접근하지만 캐시 미스가 나도 큰 문제가 없는 키를 작은 풀에 할당하고, 자주 접근하고 캐시 미스가 나면 비싼 연산을 수행해야 하는 키를 조금 더 큰 풀에 담을 수 있다. 이로써 보다 적합한 키를 Eviction 처리할 수 있게 된다.</p>
<h3 id="Replication-Within-Pools"><a href="#Replication-Within-Pools" class="headerlink" title="Replication Within Pools"></a>Replication Within Pools</h3><p>어떤 풀들은 데이터 복제를 사용하고 있다. 다음과 같은 키는 복제를 사용한다.</p>
<ul>
<li>앱에서 주기적으로 많은 키를 동시에 가져감</li>
<li>전체 데이터 셋 사이즈가 하나 혹은 두 개의 <code>Memcached</code> 서버에 딱 맞음</li>
<li>요청 속도가 한 대의 서버에서 감당하기 어려운 수준</li>
</ul>
<p>메타에서는 이런 경우 키를 나눠서 처리하는 방법 보다 복제해버리는 것을 선호한다. 예를 들어서 100개의 아이템이 있는데 다음과 같이 상황이 다른 것을 가정해 보자.</p>
<ol>
<li>키를 공평하게 둘로 나눠서 가지고 있기</li>
<li>두 개의 서버에 100개를 모두 복제</li>
</ol>
<p>요청이 1M&#x2F;s 속도로 들어오고 있고 모든 키를 가져가야 하는 경우를 생각해 보자. 공평하게 둘로 나눈 상태라면 클라이언트는 아이템을 모두 얻기 위해 두 서버 모두에게 요청을 보내게 된다. 즉 두 서버가 모두 1M&#x2F;s 부하를 받아야 하는 상황이다. 하지만 키가 두 <code>Memcached</code> 서버에 모두 동일하게 전체 셋이 복제되어 들어가 있다면 부하를 두 서버로 분산할 수 있게 된다. 단점이라고 하면 Invalidation을 두 번 해야 하는 것인데, 페이스북의 경우 요청을 분산하는 것이 Invalidation을 여러 번 처리하는 것보다 나은 선택이었다.</p>
<h2 id="장애-복구"><a href="#장애-복구" class="headerlink" title="장애 복구"></a>장애 복구</h2><p>페이스북은 <code>Memcache</code> 장애를 두 가지 스케일의 장애로 나눠서 처리했다.</p>
<ul>
<li>작은 장애: 몇 개의 서버가 영향을 받는 장애</li>
<li>큰 장애: 클러스터 내의 꽤 큰 퍼센트의 서버가 영향을 받는 장애</li>
</ul>
<p>큰 장애는 그냥 다른 클러스터로 요청을 옮기는 형태로 장애를 복구한다. 작은 장애는 보통 자동 복구에 의존하는데 이런 시스템에 의한 복구는 시간이 걸린다. 그런데 이러면 장애가 전파될 수 있는 상황이 발생할 수 있으므로 이를 막기 위한 메커니즘으로 <code>Gutter</code>를 도입했다. <code>Gutter</code>는 장애를 복구하기 위해 사용하는 전용 머신이다. 이 전용 머신은 클러스터 내에 약 1%를 차지한다. 클라이언트가 서버로부터 응답이 없으면 서버가 죽었다고 판단하고 <code>Gutter</code>에게 요청을 보낸다. <code>Gutter</code>에서 캐시 미스가 발생하면 DB에서 값을 쿼리하고 데이터를 <code>Gutter</code>에 집어넣는다.</p>
<p>이는 살아남은 다른 캐시 서버에 Rehashing을 해서 값을 채우는 방식과는 차이가 있다. 살아남은 캐시 서버에 값을 넣는 것은 다른 서버로 장애가 전파될 위험이 있다. 죽은 서버는 내부에 있던 부하가 높은 키를 가지고 있을 수 있는데, 이 키가 다른 서버로 전달되어 다른 서버의 과부하로 이어질 수 있다. 그래서 아예 유휴 서버를 두고 위험을 제한하는 방법을 사용하는 것이다.</p>
<h1 id="Multi-Clusters"><a href="#Multi-Clusters" class="headerlink" title="Multi-Clusters"></a>Multi-Clusters</h1><p><img src="/images/2023-06-23-scaling-memcache-at-facebook/region.png?style=centerme" alt="Region"><br>한 클러스터 안에서 <code>Memcached</code> 서버 수를 늘리는 것은 단순해 보이지만 온전한 해결책이 아니다. 장애 전파나 Incast Congestion을 피할 수 없게 될 수 있다. 따라서 <code>Memcached</code>를 복수의 클러스터로 만드는 방법을 선택했다. 이렇게 복수의 FE 클러스터와 하나의 스토리지 클러스터가 합쳐져서 Region을 구성한다.</p>
<blockquote>
<p>논문에서 Web Server와 <code>Memcached</code> 클러스터가 있는 것을 FE 클러스터라고 부른다. </p>
</blockquote>
<h2 id="Regional-Invalidations"><a href="#Regional-Invalidations" class="headerlink" title="Regional Invalidations"></a>Regional Invalidations</h2><p>스토리지 클러스터가 FE의 <code>Memcache</code>와 데이터 정합성을 맞추기 위한 Invalidation 책임을 가지고 있다. 이를 위해 메타에서는 <code>mcsqueal</code>이라고 하는 Invalidation Daemon을 사용한다. 이 프로세스는 CDC 형태로 DB의 Delete 요청을 분석해 FE 클러스터에게 알려준다.<br><img src="/images/2023-06-23-scaling-memcache-at-facebook/mcsqueal.png?style=centerme" alt="mcsqueal"><br>최적화를 위해 수정 요청을 보낸 웹서버도 자신의 클러스터 안에 있는 <code>Memcache</code>로 Invalidation 요청을 보낸다. 이로써 한 유저가 쓰기 후 읽기 작업을 할 때 보다 유의미한 결과를 전달해 줄 수 있게 된다.</p>
<h2 id="Regional-Pools"><a href="#Regional-Pools" class="headerlink" title="Regional Pools"></a>Regional Pools</h2><p>여러 클러스터 유저의 요청이 라우팅이 섞이면서 중복된 데이터들이 자동으로 여러 클러스터 안에 속하게 된다. 이는 클러스터 운영을 위한 캐시 중단을 만들었을 때도 다른 클러스터에 의해 Cache Hit가 줄어들지 않게 되는 등, 복제에 의한 장점이 생긴다. 하지만 문제는 메모리 비효율이 크다는 점이다.</p>
<p>이러한 메모리 비효율 문제를 해결하기 위해 <strong>Regional Pool</strong>를 적용했다. Regional Pool은 같은 <code>Memcached</code> 서버를 갖는 FE 클러스터를 의미한다.<br><img src="/images/2023-06-23-scaling-memcache-at-facebook/regional-pool.png?style=centerme" alt="Regional Pool"><br>복제는 위에서 언급했던 것처럼 Failure Tolerance, 클러스터 안의 낮은 지연 등의 효과를 가지고 있지만 어떤 경우는 이렇게 하나의 캐시 데이터를 사용하는 경우가 나은 경우가 있다. 어떤 데이터와 웹서버를 Regional Pool에 옮겨야 하는지는 경험적인 수작업에 의해 진행된다. 요구되는 데이터 접근 속도, 데이터 사이즈, 특정 아이템에 접근하는 유니크한 유저의 숫자 등 여러 지표를 룰 베이스로 판단해 옮겨 넣는다.</p>
<blockquote>
<p>마찬가지로 Regional Pool의 <code>Memcache</code>는 위에서 언급한 <strong>Gutter</strong>, <code>mcqueal</code>, <code>mcrouter</code> 등의 시스템을 모두 사용한다.</p>
</blockquote>
<h2 id="Cold-Cluster-Warm-up"><a href="#Cold-Cluster-Warm-up" class="headerlink" title="Cold Cluster Warm up"></a>Cold Cluster Warm up</h2><p>Cold 클러스터를 Warm up 할 때는 Cache Miss 발생 시 스토리지 대신 Warm 클러스터에서 가져온다. 이런 방법으로 앞서 말한 FE 클러스터 간 데이터 복제 효과도 만들 수 있으며 스토리지를 사용한 것보다 빠르게 가져올 수 있다.</p>
<p>하지만 이 방법으로 인한 Race Condition이 발생할 수 있는데, 예를 들어 Cold 클러스터에서 삭제한 다음 곧바로 Cold 클러스터에서 해당 값을 읽는 상황을 생각해 보자. 방금 삭제되었기 때문에 없어야 맞는 값인데 이  Warm 클러스터와 데이터가 동기화되지 않은 상태로 Warm 클러스터에서 값을 가져온다면 Cold 클러스터의 이 값은 언제 끝날지 모르는 불일치가 발생한 상황이 된다.</p>
<p>이를 해결하기 위해 <code>Memcached</code>에서 키 삭제 요청을 처리한 다음 해당 키에 값을 추가하는 작업을 거부하는 기능을 사용했다. 이를 <strong>Hold-Off</strong>라고 하는데 Cold 클러스터는 2초의 Hold-Off 시간을 가지고 있다. 따라서 만약 어떤 키를 Warm 클러스터로부터 가져오려고 할 때 <code>PUT</code> 요청이 실패한다면 DB에 변경이 발생했다는 것을 알 수 있고, 이 경우는 DB에서 값을 가져오도록 되어있다.</p>
<p>이런 일관성 문제가 발생할 수 있지만 어찌 됐든 Warm up 방식이 그것보다 훨씬 큰 장점을 가져다준다. Cold 클러스터의 Cache Hit이 안정되면 Warm up을 종료하고 다른 클러스터처럼 동작하게 된다.</p>
<h1 id="Multi-Regions"><a href="#Multi-Regions" class="headerlink" title="Multi-Regions"></a>Multi-Regions</h1><p>이전 챕터의 Region은 하나의 데이터 센터이다. 보통 페이스북 사이즈가 되면 데이터 센터를 대륙 혹은 지역 단위로 확장한다. 이를 통해 다음과 같은 장점을 얻을 수 있다.</p>
<ul>
<li>클라이언트와 데이터 센터를 가까이 두어 지연을 줄인다.</li>
<li>특정 지역의 자연재해, 대규모 정전 등에 영향을 완화한다.</li>
<li>새로운 장소가 더 저렴한 전력, 경제적 장점 등을 줄 수 있다.</li>
</ul>
<p><img src="/images/2023-06-23-scaling-memcache-at-facebook/multi-regions.png?style=centerme" alt="Multi-Regions"><br>각 Region은 스토리지 클러스터와 몇 개의 FE 클러스터로 구성된다. 한 Region을 마스터 데이터베이스를 가진 Region으로 지정하고 다른 Region을 Read-Only Replica(이하 Replica)로 구성한다. 이 구성에서는 <code>Memcache</code> 혹은 스토리지 클러스터에 접근하는 경우 지연이 짧다.</p>
<p>여러 Region을 운영하게 되면 일단 스토리지와 <code>Memcache</code>의 데이터 일관성을 유지하기 어려워진다. 어려운 원인은 마스터 데이터베이스에서 데이터를 가져올 때 발생하는 지연(Lag) 현상이다. 보통 이런 시스템은 일관성과 성능을 어떻게 Trade-Off 할 것인지 광범위한 스펙트럼이 있고 메타 역시 이 스펙트럼의 어떤 한 지점을 고른 것이다. 이는 서비스의 특징 및 규모에 따라 경험적으로 선택되고 논문에서는 꽤 받아들일 수 있는 수준의 Trade-Off를 찾았다고 설명한다.</p>
<h2 id="Master-Region에서-쓰는-경우"><a href="#Master-Region에서-쓰는-경우" class="headerlink" title="Master Region에서 쓰는 경우"></a>Master Region에서 쓰는 경우</h2><p>Master Region은 이전에 설명한 한 Region에서 쓰기가 발생했을 때 <code>mcsqueal</code>이 동작하는 방법대로 동작한다. 하지만 이 Daemon 프로세스의 동작은 클러스터 안에서 한정된다. 다른 Region의 <code>Memcache</code>에 Invalidation을 전파하는 것은 동시성 이슈를 만들 수 있다. 예를 들어서 데이터가 수정되어 DB Replication이 발생해야 하는데, 이 데이터보다 Cache Invalidation이 먼저 도착하게 되고, 곧바로 클라이언트가 해당 키를 읽었다고 가정해 보자. 그러면 클라이언트는 해당 키에서 값을 못 찾고 Region 안에 있는 스토리지 클러스터에서 데이터를 찾게 된다. 그러면 오래된 데이터가 다시 캐시되고 유저는 오래된 데이터를 보게 된다.</p>
<h2 id="Non-Master-Region에서-쓰는-경우"><a href="#Non-Master-Region에서-쓰는-경우" class="headerlink" title="Non-Master Region에서 쓰는 경우"></a>Non-Master Region에서 쓰는 경우</h2><p>복제 지연이 발생하고 있는 상황에서 Non-Master Region에서 데이터를 업데이트 한다고 가정해 보자. Region의 <code>Memcache</code>에 Invalidation을 했든 안 했든, Master 데이터베이스가 변경된 값을 Replica로 전달하지 않은 상태라면 업데이트를 요청한 유저가 오래된 데이터를 읽어오게 되는 상황이 생길 수 있다.</p>
<blockquote>
<p><code>Name = &quot;changhoi&quot;</code>라고 수정하고 새로고침 된 페이지에서 여전히 <code>&quot;CHANGHOI&quot;</code>라고 보이는 상황을 의미한다.</p>
</blockquote>
<p>따라서 Replica에서 데이터를 캐시에 채울 수 있는 순간은 복제 스트림을 따라잡고 난 다음이어야 한다. 만약 복제 스트림을 따라잡지 못한 상태라면 웹서버는 데이터를 Master Region 스토리지 클러스터에서 가져온다.</p>
<p>이 동작을 위해 <strong>Remote Marker</strong>를 도입했다. 한 서버가 <code>K</code>라는 키에 영향을 주는 업데이트를 한다면 다음과 같은 순서를 따른다.</p>
<ol>
<li>Region 안에 Remote Marker를 <code>R(K)</code>에 둔다.</li>
<li><code>K</code>와 <code>R(K)</code>를 SQL 구문 안에서 Invalidation 될 수 있도록 포함시켜 마스터에 쓰기를 수행한다.</li>
<li>Region의 <code>Memcache</code>에서 <code>K</code>를 삭제한다.</li>
</ol>
<blockquote>
<p>2번 단계가 구체적으로 이해가 잘 안되는데, SQL 구문이 Replica에 전파될 때 <code>R(K)</code>를 같이 없앨 수 있게 SQL 구문에 내장한다는 느낌이었다. </p>
</blockquote>
<p>이렇게 동작하면 Cache Miss가 발생했을 때 <code>K</code>에 대한 마커가 남아있는 경우 Region의 Replica에서 아직 오래된 데이터를 가지고 있다는 뜻이 되므로 Master Region의 스토리지에서 데이터를 가져온다. 만약 마커가 없다면 Region 안에 있는 Replica에서 값을 가져온다.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf">Scaling Memcache at Facebook</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/watch?v=m4_7W4XzRgk">Scaling Memcache at Facebook Youtube</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://nymets.medium.com/%EB%B2%88%EC%97%AD-scaling-memcache-at-facebook-9c67f9e61282">Scaling Memcache at Facebook 요약 &amp; 해석</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Scaling Memcache At Facebook</p><p><a href="https://changhoi.kim/posts/database/scaling-memcache-at-facebook/">https://changhoi.kim/posts/database/scaling-memcache-at-facebook/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>changhoi</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-06-23</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-06-23</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="external nofollow noopener noreferrer" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="external nofollow noopener noreferrer" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="external nofollow noopener noreferrer" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/system-design/">system_design</a><a class="link-muted mr-2" rel="tag" href="/tags/memcache/">memcache</a></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" target="_blank" rel="external nofollow noopener noreferrer" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><div class="card"><div class="card-content"><h3 class="title is-5">관련 글</h3><article class="media"><figure class="media-left"><a class="image" href="/posts/database/etcd-client-model/"><img src="/images/2022-11-03-etcd-data-model/thumbnail.png" alt="etcd deep dive - Client Model"></a></figure><div class="media-content"><p class="date"><time datetime="2022-12-10T15:00:00.000Z">2022-12-11</time></p><p class="title"><a href="/posts/database/etcd-client-model/">etcd deep dive - Client Model</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/posts/database/etcd-data-model/"><img src="/images/2022-11-03-etcd-data-model/thumbnail.png" alt="etcd deep dive - Data Model"></a></figure><div class="media-content"><p class="date"><time datetime="2022-11-02T15:00:00.000Z">2022-11-03</time></p><p class="title"><a href="/posts/database/etcd-data-model/">etcd deep dive - Data Model</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/posts/database/dynamodb-internals-2/"><img src="/images/2022-04-16-dynamodb-internals-1/thumbnail.png" alt="DynamoDB Internals (2) - DynamoDB"></a></figure><div class="media-content"><p class="date"><time datetime="2022-04-18T15:00:00.000Z">2022-04-19</time></p><p class="title"><a href="/posts/database/dynamodb-internals-2/">DynamoDB Internals (2) - DynamoDB</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/posts/database/dynamodb-internals-1/"><img src="/images/2022-04-16-dynamodb-internals-1/thumbnail.png" alt="DynamoDB Internals (1) - Dynamo"></a></figure><div class="media-content"><p class="date"><time datetime="2022-04-15T15:00:00.000Z">2022-04-16</time></p><p class="title"><a href="/posts/database/dynamodb-internals-1/">DynamoDB Internals (1) - Dynamo</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/posts/database/rdb-scaling/"><img src="/images/2022-02-09-rdb-scaling/thumbnail.png?style=centerme" alt="RDB 스케일링"></a></figure><div class="media-content"><p class="date"><time datetime="2022-02-08T15:00:00.000Z">2022-02-09</time></p><p class="title"><a href="/posts/database/rdb-scaling/">RDB 스케일링</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/posts/database/mongodb-modeling/"><img src="https://cdn.iconscout.com/icon/free/png-512/mongodb-226029.png" alt="MongoDB 모델링"></a></figure><div class="media-content"><p class="date"><time datetime="2021-05-07T15:00:00.000Z">2021-05-08</time></p><p class="title"><a href="/posts/database/mongodb-modeling/">MongoDB 모델링</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/posts/database/dynamodb-single-table-design/"><img src="/images/2022-04-16-dynamodb-internals-1/thumbnail.png" alt="DynamoDB 설계 방법: Single Table Design"></a></figure><div class="media-content"><p class="date"><time datetime="2020-10-23T15:00:00.000Z">2020-10-24</time></p><p class="title"><a href="/posts/database/dynamodb-single-table-design/">DynamoDB 설계 방법: Single Table Design</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article></div></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/go/go-pkg-architecture-theory/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Go Package Architecture - 이론편</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/network/about-http3/"><span class="level-item">HTTP/3에 대하여</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">댓글</h3><div class="fb-comments" data-width="100%" data-href="https://changhoi.kim/posts/database/scaling-memcache-at-facebook/" data-num-posts="5"></div><script>(function(d, s, id) {
            var js, fjs = d.getElementsByTagName(s)[0];
            if (d.getElementById(id)) return;
            js = d.createElement(s); js.id = id;
            js.src = "//connect.facebook.net/ko/sdk.js#xfbml=1&version=v12.0";
            fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars2.githubusercontent.com/u/34329147?s=400&amp;u=9d1b1dc88bc8ee5965f35b0160780264c3619eb6&amp;v=4" alt="Changhoi Kim"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Changhoi Kim</p><p class="is-size-6 is-block">Co-Founder of Bayesians</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul, Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives/"><p class="title">73</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories/"><p class="title">13</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags/"><p class="title">47</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://www.linkedin.com/in/changhoi/" target="_blank" rel="external nofollow noopener noreferrer">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="LinkedIn" href="https://www.linkedin.com/in/changhoi/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Github" href="https://github.com/changhoi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Email" href="mailto:changhoi0522@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="English" href="https://jayce.kim"><i class="fas fa-language"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/backend/"><span class="level-start"><span class="level-item">backend</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/books/"><span class="level-start"><span class="level-item">books</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/docker/"><span class="level-start"><span class="level-item">docker</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/essay/"><span class="level-start"><span class="level-item">essay</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/etc/"><span class="level-start"><span class="level-item">etc</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/go/"><span class="level-start"><span class="level-item">go</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/linux/"><span class="level-start"><span class="level-item">linux</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/logs/"><span class="level-start"><span class="level-item">logs</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/os/"><span class="level-start"><span class="level-item">os</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/rust/"><span class="level-start"><span class="level-item">rust</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/serverless/"><span class="level-start"><span class="level-item">serverless</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Overview"><span class="level-left"><span class="level-item">1</span><span class="level-item">Overview</span></span></a></li><li><a class="level is-mobile" href="#Single-Cluster"><span class="level-left"><span class="level-item">2</span><span class="level-item">Single Cluster</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#지연-줄이기"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">지연 줄이기</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Parallel-Requests-And-Batching"><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">Parallel Requests And Batching</span></span></a></li><li><a class="level is-mobile" href="#Client-Server-Communication"><span class="level-left"><span class="level-item">2.1.2</span><span class="level-item">Client-Server Communication</span></span></a></li><li><a class="level is-mobile" href="#Incast-Congestion"><span class="level-left"><span class="level-item">2.1.3</span><span class="level-item">Incast Congestion</span></span></a></li></ul></li><li><a class="level is-mobile" href="#부하-줄이기"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">부하 줄이기</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Lease"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">Lease</span></span></a></li><li><a class="level is-mobile" href="#Memcache-Pools"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Memcache Pools</span></span></a></li><li><a class="level is-mobile" href="#Replication-Within-Pools"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">Replication Within Pools</span></span></a></li></ul></li><li><a class="level is-mobile" href="#장애-복구"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">장애 복구</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Multi-Clusters"><span class="level-left"><span class="level-item">3</span><span class="level-item">Multi-Clusters</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Regional-Invalidations"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Regional Invalidations</span></span></a></li><li><a class="level is-mobile" href="#Regional-Pools"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Regional Pools</span></span></a></li><li><a class="level is-mobile" href="#Cold-Cluster-Warm-up"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Cold Cluster Warm up</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Multi-Regions"><span class="level-left"><span class="level-item">4</span><span class="level-item">Multi-Regions</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Master-Region에서-쓰는-경우"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Master Region에서 쓰는 경우</span></span></a></li><li><a class="level is-mobile" href="#Non-Master-Region에서-쓰는-경우"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Non-Master Region에서 쓰는 경우</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">5</span><span class="level-item">Reference</span></span></a></li></ul></div></div><script src="/js/toc.js" defer></script></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.jpg" alt="Changhoi Kim" height="28"></a><p class="is-size-7"><span>&copy; 2025 changhoi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" target="_blank" rel="external nofollow noopener noreferrer" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>